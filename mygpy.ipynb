{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def get_ids(ids,counts=None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids,ids[1:]):\n",
    "        counts[pair] = counts.get(pair,0)+1\n",
    "    return counts\n",
    "\n",
    "def merge(ids,top_pair,idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids)-1 and ids[i] == top_pair[0] and ids[i+1]==top_pair[1]:\n",
    "            newids.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i+=1\n",
    "    return newids\n",
    "\n",
    "\n",
    "def replace_control_characters(s:str) -->str:\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] !=\"C\":\n",
    "            chars.append(ch)\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\")\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_tokens(t:bytes) -->str:\n",
    "    s = t.decode('utf-8',errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "Class BasicTokenizer():\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.patterns = \"\"\n",
    "        self.special_tokens ={}\n",
    "        self.vocab_size = self._build_vocab()\n",
    "    \n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        raise NotImplementedError    \n",
    "    \n",
    "    def encode(self,text):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        \n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0,p1), idx in self.merge.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special,idx in self.special_tokens.items():\n",
    "            vocab[idx]  = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "    \n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char ï¿½.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()\n",
    "            \n",
    "            \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
