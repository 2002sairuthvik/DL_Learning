{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOOPykAzuKXIajgBz11jINV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wc0bWT65JdBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc7JFnodAwIZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5ec7qGQ9A876"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "mrscc_dir = '/content/drive/MyDrive/mrscc'\n",
        "questions = pd.read_csv(os.path.join(mrscc_dir, 'testing_data.csv'))\n",
        "answers = pd.read_csv(os.path.join(mrscc_dir, 'test_answer.csv'))"
      ],
      "metadata": {
        "id": "NEQV6nOUBIuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "VhZzEwySFvIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers"
      ],
      "metadata": {
        "id": "hBvV8UhwGE_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying model for MLM taks"
      ],
      "metadata": {
        "id": "m6GPT1FoFn5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "choices = {'a)':1, 'b)':2, 'c)':3, 'd)':4, 'e)':5}\n",
        "model_name = 'distilroberta-base'"
      ],
      "metadata": {
        "id": "vpKfVjqeFlbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModelEvaluator():\n",
        "\n",
        "  def __init__(self, q, a, c, mn):\n",
        "    self.questions, self.answers, self.choices, self.model_name = q, a, c, mn\n",
        "    print(len(self.questions))\n",
        "    self.process_questions_and_answers()\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "    self.sent_encodings, self.word_encodings, self.mask_idxs = self.make_encodings()\n",
        "\n",
        "  def run_model_and_evaluate(self):\n",
        "    output = self.make_predictions()\n",
        "    self.accuracy = self.get_model_accuracy(output, self.questions['answer'])\n",
        "\n",
        "  def process_questions_and_answers(self, s='_____'):\n",
        "    answer_idxs, candidate_questions = [], []\n",
        "    for index, row in self.questions.iterrows():\n",
        "      answer = answers.iloc[index].answer + ')'\n",
        "      answer_idxs.append(self.choices.get(answer))\n",
        "      candidate_questions.append([re.sub(s, row.loc[c], row.loc['question']) for c in self.choices.keys()])\n",
        "    self.questions.loc[:, 'candidate_questions'] = candidate_questions\n",
        "    self.questions.loc[:, 'answer'] = answer_idxs\n",
        "\n",
        "  def get_sublist_idxs_in_list(self, word, sentence):\n",
        "    # find mask indicies for encoded sentence\n",
        "    possibles = np.where(sentence == word[0])[0]\n",
        "    for p in possibles:\n",
        "      check = sentence[p:p + len(word)]\n",
        "      if np.all(check == word):\n",
        "          return list(range(p, (p + len(word))))\n",
        "\n",
        "  def make_encodings(self):\n",
        "    sent_encodings, word_encodings, mask_idxs = [], [], []\n",
        "    for index, row in self.questions.iterrows():\n",
        "        _sent_encodings, _word_encodings, _mask_idxs = [], [], []\n",
        "        for i, (word, sentence) in enumerate(zip(row[self.choices.keys()], row.loc['candidate_questions'])):\n",
        "          encoded_word = self.tokenizer.encode(str(\" \" + word), add_special_tokens=False)\n",
        "          encoded_sent = self.tokenizer.encode_plus(sentence, add_special_tokens=True, return_tensors='pt',\n",
        "                                                padding='max_length', max_length=128, return_attention_mask=True)\n",
        "          tokens_to_mask_idx = self.get_sublist_idxs_in_list(np.array(encoded_word), np.array(encoded_sent['input_ids'][0]))\n",
        "          encoded_sent['input_ids'][0][tokens_to_mask_idx] = self.tokenizer.mask_token_id\n",
        "          _sent_encodings.append(encoded_sent)\n",
        "          _word_encodings.append(encoded_word)\n",
        "          _mask_idxs.append(tokens_to_mask_idx)\n",
        "        sent_encodings.append(_sent_encodings)\n",
        "        word_encodings.append(_word_encodings)\n",
        "        mask_idxs.append(_mask_idxs)\n",
        "    return sent_encodings, word_encodings, mask_idxs\n",
        "\n",
        "  def make_predictions(self):\n",
        "    output = []\n",
        "    for q_idx, (w, s, m) in enumerate(zip(self.word_encodings, self.sent_encodings, self.mask_idxs)):\n",
        "      print(f'Question {q_idx}')\n",
        "      predictions = []\n",
        "      candidate_input_ids = torch.stack([inp_ids['input_ids'].squeeze(0) for inp_ids in s])\n",
        "      candidate_attention_masks = torch.stack([am['attention_mask'].squeeze(0) for am in s])\n",
        "      candidate_logits = self.model(candidate_input_ids, attention_mask=candidate_attention_masks).logits\n",
        "      for idx, (token, mask_idxs) in enumerate(zip(w, m)):\n",
        "        mask_token_logits = candidate_logits[idx, mask_idxs, token]\n",
        "        candidate_score = float(torch.mean(mask_token_logits))\n",
        "        predictions.append(candidate_score)\n",
        "      output.append(np.argmax(predictions) + 1)\n",
        "    return output\n",
        "\n",
        "  def get_model_accuracy(self, predictions, ground_truth):\n",
        "    correct = 0\n",
        "    for pred, gt in zip(predictions, ground_truth):\n",
        "      if pred == gt:\n",
        "        correct += 1\n",
        "    return correct/len(ground_truth)\n"
      ],
      "metadata": {
        "id": "lBBjM2pLHeZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = LanguageModelEvaluator(questions[:100],answers,choices,model_name)"
      ],
      "metadata": {
        "id": "CC5L45W4XXLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.get_model_accuracy(evaluator.make_predictions(), evaluator.questions['answer'])"
      ],
      "metadata": {
        "id": "quk8xJgzYMWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processfiles(files, train_dir):\n",
        "  texts = []\n",
        "  for i, a_file in enumerate(files):\n",
        "    text = \"\"\n",
        "    try:\n",
        "      with open(os.path.join(train_dir,a_file)) as instream:\n",
        "        for line in instream:\n",
        "          text += line\n",
        "          texts.append(text)\n",
        "    except UnicodeDecodeError:\n",
        "      print(f\"Unicode error for this file: {a_file}\")\n",
        "  return texts"
      ],
      "metadata": {
        "id": "_nj5_Z1Karj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "file_names = glob.glob('/content/drive/MyDrive/Holmes_Training_Data/*')\n",
        "sample_files = file_names[:100]"
      ],
      "metadata": {
        "id": "NkimDM-iQVja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = processfiles(sample_files, mrscc_dir)"
      ],
      "metadata": {
        "id": "4C6G6ydPMPWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "GWvU6YzcUIQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "yf-HK_hHN04c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_len = 5\n",
        "texts_dict = {'text':[t for t in texts[:ds_len]]}"
      ],
      "metadata": {
        "id": "ihk7aTfhOSH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_dict(texts_dict)"
      ],
      "metadata": {
        "id": "i3CqkVw6O5vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "q30RxYzvO_O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilroberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_ds = ds.map(lambda batch : tokenizer(batch['text'], remove_columns=['text']), batched=True, num_proc=2)"
      ],
      "metadata": {
        "id": "e7AmUDtjPCPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "\n",
        "def group_texts(text):\n",
        "  concat_text = {k:sum(texts[k],[]) for k in texts.keys()}\n",
        "  total_length = len(concat_text[list(text.keys())[0]])\n",
        "  total_length = (total_length // block_size) * block_size\n",
        "  result = {k:concat_text[k][:total_length] for k in concat_text.keys()}\n",
        "  result['labels'] = result['input_ids'].copy()\n",
        "  return result"
      ],
      "metadata": {
        "id": "3daBpD0qScWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c097389c"
      },
      "source": [
        "lm_dataset = tokenized_ds.map(group_texts, batched=True, num_proc=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}